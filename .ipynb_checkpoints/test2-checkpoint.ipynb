{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9efb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d672f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the datasets \n",
    "path = '/Users/dan/YTTnT/archive_1/'\n",
    "df1 = pd.read_csv(path + 'USvideos.csv')\n",
    "df2 = pd.read_csv(path + 'CAvideos.csv')\n",
    "df3 = pd.read_csv(path + 'GBvideos.csv')\n",
    "\n",
    "#load the datasets containing the category names\n",
    "data1 = json.load(open(path + 'US_category_id.json'))\n",
    "data2 = json.load(open(path + 'CA_category_id.json'))\n",
    "data3 = json.load(open(path + 'GB_category_id.json'))\n",
    "\n",
    "path1 = '/Users/dan/YTTnT/archive/'\n",
    "\n",
    "df4 = pd.read_csv(path1 + 'US_youtube_trending_data.csv')\n",
    "df5 = pd.read_csv(path1 + 'CA_youtube_trending_data.csv')\n",
    "df6 = pd.read_csv(path1 + 'GB_youtube_trending_data.csv')\n",
    "\n",
    "data4 = json.load(open(path1 + 'US_category_id.json'))\n",
    "data5 = json.load(open(path1 + 'CA_category_id.json'))\n",
    "data6 = json.load(open(path1 + 'GB_category_id.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06f0b3e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5692\\3411008121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#create a new category column by mapping the category names to their id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category_title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category_title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category_title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "def category_extractor(data):\n",
    "    i_d = [data['items'][i]['id'] for i in range(len(data['items']))]\n",
    "    title = [data['items'][i]['snippet'][\"title\"] for i in range(len(data['items']))]\n",
    "    i_d = list(map(int, i_d))\n",
    "    category = zip(i_d, title)\n",
    "    category = dict(category)\n",
    "    return category\n",
    "\n",
    "#create a new category column by mapping the category names to their id\n",
    "df1['category_title'] = df1['category_id'].map(category_extractor(data1))\n",
    "df2['category_title'] = df2['category_id'].map(category_extractor(data2))\n",
    "df3['category_title'] = df3['category_id'].map(category_extractor(data3))\n",
    "df4['category_title'] = df4['categoryId'].map(category_extractor(data4))\n",
    "df5['category_title'] = df5['categoryId'].map(category_extractor(data5))\n",
    "df6['category_title'] = df6['categoryId'].map(category_extractor(data6))\n",
    "\n",
    "target_category = 'Science & Technology'\n",
    "\n",
    "#join the dataframes\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "\n",
    "#drop rows based on duplicate videos\n",
    "df = df.drop_duplicates('video_id')\n",
    "\n",
    "#collect only titles of entertainment videos\n",
    "#feel free to use any category of video that you want\n",
    "data = df[df['category_title'] == target_category]\n",
    "#entertainment = df[df['category_title'] == 'Entertainment']['title']\n",
    "entertainment = data['title']\n",
    "entertainment = entertainment.tolist()\n",
    "#remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    text = ''.join(e for e in text if e not in string.punctuation).lower()\n",
    "    \n",
    "    text = text.encode('utf8').decode('ascii', 'ignore')\n",
    "    return text\n",
    "\n",
    "corpus = [clean_text(e) for e in entertainment]\n",
    "\n",
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del df4\n",
    "del df5\n",
    "del df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f81e1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "apposV2 = {\n",
    "\"are not\" : \"are not\",\n",
    "\"ca\" : \"can\",\n",
    "\"could n't\" : \"could not\",\n",
    "\"did n't\" : \"did not\",\n",
    "\"does n't\" : \"does not\",\n",
    "\"do n't\" : \"do not\",\n",
    "\"had n't\" : \"had not\",\n",
    "\"has n't\" : \"has not\",\n",
    "\"have n't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"is n't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"might n't\" : \"might not\",\n",
    "\"must n't\" : \"must not\",\n",
    "\"sha\" : \"shall\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"should n't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"were n't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"wo\" : \"will\",\n",
    "\"would n't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"was n't\": \"was not\",\n",
    "\"we'll\":\"we will\",\n",
    "\"did n't\": \"did not\"\n",
    "}\n",
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21993714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0637f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dan\\AppData\\Local\\Temp\\ipykernel_5692\\624157640.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['title_cleaned'] = cleanData(X)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\",disable=['parser','ner'])\n",
    "stop = stopwords.words('english')\n",
    "def cleanData(titles):\n",
    "    all_=[]\n",
    "    for title in titles:\n",
    "        lower_case = title.lower() #lower case the text\n",
    "        lower_case = lower_case.replace(\" n't\",\" not\") #correct n't as not\n",
    "        lower_case = lower_case.replace(\".\",\" . \")\n",
    "        lower_case = ' '.join(word.strip(string.punctuation) for word in lower_case.split()) #remove punctuation\n",
    "        words = lower_case.split() #split into words\n",
    "        words = [word for word in words if word.isalpha()] #remove numbers\n",
    "        split = [apposV2[word] if word in apposV2 else word for word in words] #correct using apposV2 as mentioned above\n",
    "        split = [appos[word] if word in appos else word for word in split] #correct using appos as mentioned above\n",
    "        split = [word for word in split if word not in stop] #remove stop words\n",
    "        reformed = \" \".join(split) #join words back to the text\n",
    "        doc = nlp(reformed)\n",
    "        reformed = \" \".join([token.lemma_ for token in doc]) #lemmatiztion\n",
    "        all_.append(reformed)\n",
    "    return all_\n",
    "data['title_cleaned'] = cleanData(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c813940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dan\\AppData\\Local\\Temp\\ipykernel_5692\\1621386908.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"split\"] = data.apply(lambda x: \"train\" if random.randrange(0,100) > 10 else \"valid\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "for category in cat.values(): \n",
    "    cat_str = str(category.strip(string.punctuation)).split()[0]\n",
    "    cat_str = cat_str.split('/')[0]\n",
    "    titles = cleanTitles(df['title'][df['category_title'] == category])\n",
    "    np.savetxt('Data/' + cat_str + '.csv', np.array(title_wout_stop), fmt='%s')\n",
    "    #data = df['cleaned_title'][df['category_title'] == category]\n",
    "    i = 0\n",
    "    for ele in data:\n",
    "        if ele is not None:\n",
    "            i = i + 1\n",
    "    if i > 100:\n",
    "        tokenizer = Tokenizer()\n",
    "        get_sequence_of_tokens(titles, tokenizer)\n",
    "        np.savetxt('Data/' + cat_str + '_keys.csv', np.array(list(tokenizer.word_index.keys())), fmt='%s')\n",
    "    else:\n",
    "        print('Insufficient # of Samples ' + cat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "inp_sequences, total_words = get_sequence_of_tokens(title, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(corpus, tokenizer):\n",
    "    #get tokens\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #convert to sequence of tokens\n",
    "    max_sequence = 0\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        #print(line)\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        if len(token_list) > max_sequence:\n",
    "            max_sequence = len(token_list)\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences, total_words, max_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7042416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences, total_words):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,  maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:, -1]\n",
    "    label = ku.to_categorical(label, num_classes = total_words)\n",
    "    return predictors, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a69fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "    # Add Hidden Layer 1 — LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121147a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):\n",
    "    \n",
    "    original = seed_text\n",
    "    \n",
    "    lower_case = seed_text.lower() #lower case the text\n",
    "    lower_case = lower_case.replace(\" n't\",\" not\") #correct n't as not\n",
    "    lower_case = lower_case.replace(\".\",\" . \")\n",
    "    lower_case = ' '.join(word.strip(string.punctuation) for word in lower_case.split()) #remove punctuation\n",
    "    words = lower_case.split() #split into words\n",
    "    words = [word for word in words if word.isalpha()] #remove numbers\n",
    "    split = [apposV2[word] if word in apposV2 else word for word in words] #correct using apposV2 as mentioned above\n",
    "    split = [appos[word] if word in appos else word for word in split] #correct using appos as mentioned above\n",
    "    split = [word for word in split if word not in stop] #remove stop words\n",
    "    \n",
    "    \n",
    "    reformed = \" \".join(split) #join words back to the text\n",
    "    doc = nlp(reformed)\n",
    "    reformed = \" \".join([token.lemma_ for token in doc]) #lemmatiztion\n",
    "\n",
    "    seed_text = reformed\n",
    "    \n",
    "    previous = ''\n",
    "    \n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1,  padding='pre')\n",
    "    predict_x = model.predict(token_list) \n",
    "    predicted_list = np.argsort(predict_x[0])[-next_words:][::-1]\n",
    "\n",
    "    output_word = ''\n",
    "    for predicted in predicted_list:\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += ' '+output_word\n",
    "    #output += ' ' + output_word\n",
    "    \n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_category in list(category_list):\n",
    "\n",
    "    print(target_category)\n",
    "    tokenizer = Tokenizer()\n",
    "    inp_sequences, total_words, max_sequence_len = get_sequence_of_tokens(df['cleaned_title'][df['category_title'] == target_category], tokenizer)\n",
    "    predictors, label = generate_padded_sequences(inp_sequences, total_words)\n",
    "    model_wout_stop = create_model(max_sequence_len, total_words)\n",
    "    history = model_wout_stop.fit(predictors, label, epochs=75, verbose=4)\n",
    "    plt.plot(history.history['loss'], label='Training')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    model_wout_stop.save('model_wout_stop_{0}_{1}'.format(target_category, 75))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
